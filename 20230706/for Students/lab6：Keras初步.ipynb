{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras 初步\n",
    "\n",
    "## 一、演练介绍\n",
    "\n",
    "### 1. 演练内容\n",
    "\n",
    "本演练是后续演练的基础，旨在学习基于 Python 的深度学习库——Keras。Keras 是常见深度学习库如 Theano，Tensorflow 的高级封装，Keras 本身并不具备底层运算的能力，而是一个具备这种底层运算能力的 backend（后端）协同工作。\n",
    "\n",
    "### 2. 演练技能点\n",
    "\n",
    "在本演练中，将掌握\n",
    "- Keras的基本使用方法\n",
    "- 用 Keras Sequential 顺序模型构建网络\n",
    "- 用 Keras 函数式 API 构建网络\n",
    "- 与 NLP 有关的网络层\n",
    "\n",
    "### 3. 演练要求\n",
    "\n",
    "本演练要求具备以下基本能力\n",
    "- 掌握 Python 的基本操作\n",
    "- 了解机器学习基本概念（如训练，预测，损失函数，梯度下降）\n",
    "- 了解神经网络基本网络层（如全连接层，卷积层）\n",
    "\n",
    "## 二、演练步骤\n",
    "安装需要的包："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m运行具有“kg01”的单元格需要ipykernel包。\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'f:/Venvs/kg01/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==1.13.1\n",
    "!pip install keras==2.1.6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 特别说明:\n",
    "## 1. 上述步骤，执行时间较长\n",
    "## 2.  解决包冲突问题：\n",
    "protobuf 包是 tensorflow、tensorboard 依赖的重要包，但是其升级快。\n",
    "https://pypi.org/project/protobuf/\n",
    "https://zhuanlan.zhihu.com/p/406832315\n",
    "和本演练所使用的 tensorflow 版本不匹配，产生冲突。\n",
    "解决方法如下：\n",
    "### 1. 查看当前 protobuf 的版本，执行：!pip show protobuf\n",
    "### 2. 删除当前高版本的 protobuf，执行：!pip uninstall protobuf\n",
    "### 3. 安装匹配当前 tensorflow 的（低版本）的 protobuf ，执行：!pip install protobuf==3.20\n",
    "## 3. 代码执行过程，会提示一些降级警告，可以忽略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: protobuf\n",
      "Version: 4.23.3\n",
      "Summary: \n",
      "Home-page: https://developers.google.com/protocol-buffers/\n",
      "Author: protobuf@googlegroups.com\n",
      "Author-email: protobuf@googlegroups.com\n",
      "License: 3-Clause BSD License\n",
      "Location: f:\\venvs\\kg01\\lib\\site-packages\n",
      "Requires: \n",
      "Required-by: tensorboard, tensorflow\n",
      "^C\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting protobuf==3.20\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a0/c2/1b18ce749a736fb11f8e2c153fb28544415c801455d93d41e60f3e607510/protobuf-3.20.0-cp37-cp37m-win_amd64.whl (905 kB)\n",
      "     ------------------------------------- 905.1/905.1 kB 28.0 MB/s eta 0:00:00\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.23.3\n",
      "    Uninstalling protobuf-4.23.3:\n",
      "      Successfully uninstalled protobuf-4.23.3\n",
      "Successfully installed protobuf-3.20.0\n"
     ]
    }
   ],
   "source": [
    "!pip show protobuf\n",
    "!pip uninstall protobuf\n",
    "!pip install protobuf==3.20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sequential 顺序模型\n",
    "\n",
    "Keras 的核心数据结构是 model，一种组织网络层的方式。最简单的模型是 Sequential 顺序模型，它由多个网络层线性堆叠。\n",
    "Sequential 模型如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "f:\\Venvs\\kg01\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "f:\\Venvs\\kg01\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "f:\\Venvs\\kg01\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "f:\\Venvs\\kg01\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "f:\\Venvs\\kg01\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "f:\\Venvs\\kg01\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "# 初始化 model \n",
    "model = Sequential()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化模型之后，就可以通过 `add` 方法来堆叠模型了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From f:\\Venvs\\kg01\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# 添加全连接层作为第一层，全连接层的节点数为64，激活函数为 relu，以(*, 16) 的数组作为输入\n",
    "model.add(Dense(units=64, activation='relu', input_shape=(16,)))\n",
    "# 添加第二个全连接层，节点数为1，激活函数为 sigmoid，在第一层之后就不需要指定输入尺寸了，Keras 会自动计算\n",
    "model.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在完成模型的构建后，使用 `compile` 来配置学习过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', # 定义损失函数为二元交叉熵函数\n",
    "              optimizer='sgd', # 梯度更新算法为随机梯度下降算法\n",
    "              metrics=['accuracy']) # 模型的评估标准为精度"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印模型的结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                1088      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,153\n",
      "Trainable params: 1,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在就可以用 `fit` 在训练数据上进行迭代了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From f:\\Venvs\\kg01\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/30\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.5018 - acc: 1.0000\n",
      "Epoch 2/30\n",
      "32/32 [==============================] - 0s 63us/step - loss: 0.4886 - acc: 1.0000\n",
      "Epoch 3/30\n",
      "32/32 [==============================] - 0s 31us/step - loss: 0.4759 - acc: 1.0000\n",
      "Epoch 4/30\n",
      "32/32 [==============================] - 0s 31us/step - loss: 0.4637 - acc: 1.0000\n",
      "Epoch 5/30\n",
      "32/32 [==============================] - 0s 0us/step - loss: 0.4520 - acc: 1.0000\n",
      "Epoch 6/30\n",
      "32/32 [==============================] - 0s 31us/step - loss: 0.4407 - acc: 1.0000\n",
      "Epoch 7/30\n",
      "32/32 [==============================] - 0s 33us/step - loss: 0.4298 - acc: 1.0000\n",
      "Epoch 8/30\n",
      "32/32 [==============================] - 0s 0us/step - loss: 0.4193 - acc: 1.0000\n",
      "Epoch 9/30\n",
      "32/32 [==============================] - 0s 32us/step - loss: 0.4092 - acc: 1.0000\n",
      "Epoch 10/30\n",
      "32/32 [==============================] - 0s 0us/step - loss: 0.3994 - acc: 1.0000\n",
      "Epoch 11/30\n",
      "32/32 [==============================] - 0s 35us/step - loss: 0.3900 - acc: 1.0000\n",
      "Epoch 12/30\n",
      "32/32 [==============================] - 0s 31us/step - loss: 0.3809 - acc: 1.0000\n",
      "Epoch 13/30\n",
      "32/32 [==============================] - 0s 31us/step - loss: 0.3721 - acc: 1.0000\n",
      "Epoch 14/30\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.3636 - acc: 1.0000\n",
      "Epoch 15/30\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.3553 - acc: 1.0000\n",
      "Epoch 16/30\n",
      "32/32 [==============================] - 0s 31us/step - loss: 0.3473 - acc: 1.0000\n",
      "Epoch 17/30\n",
      "32/32 [==============================] - 0s 31us/step - loss: 0.3396 - acc: 1.0000\n",
      "Epoch 18/30\n",
      "32/32 [==============================] - 0s 63us/step - loss: 0.3322 - acc: 1.0000\n",
      "Epoch 19/30\n",
      "32/32 [==============================] - 0s 31us/step - loss: 0.3250 - acc: 1.0000\n",
      "Epoch 20/30\n",
      "32/32 [==============================] - 0s 62us/step - loss: 0.3180 - acc: 1.0000\n",
      "Epoch 21/30\n",
      "32/32 [==============================] - 0s 31us/step - loss: 0.3113 - acc: 1.0000\n",
      "Epoch 22/30\n",
      "32/32 [==============================] - 0s 31us/step - loss: 0.3048 - acc: 1.0000\n",
      "Epoch 23/30\n",
      "32/32 [==============================] - 0s 84us/step - loss: 0.2985 - acc: 1.0000\n",
      "Epoch 24/30\n",
      "32/32 [==============================] - 0s 33us/step - loss: 0.2924 - acc: 1.0000\n",
      "Epoch 25/30\n",
      "32/32 [==============================] - 0s 34us/step - loss: 0.2864 - acc: 1.0000\n",
      "Epoch 26/30\n",
      "32/32 [==============================] - 0s 33us/step - loss: 0.2807 - acc: 1.0000\n",
      "Epoch 27/30\n",
      "32/32 [==============================] - 0s 31us/step - loss: 0.2751 - acc: 1.0000\n",
      "Epoch 28/30\n",
      "32/32 [==============================] - 0s 66us/step - loss: 0.2697 - acc: 1.0000\n",
      "Epoch 29/30\n",
      "32/32 [==============================] - 0s 33us/step - loss: 0.2645 - acc: 1.0000\n",
      "Epoch 30/30\n",
      "32/32 [==============================] - 0s 31us/step - loss: 0.2594 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# fit 方法接收的第一个数据是训练的输入数据，第二个数据为训练的期望输出数据\n",
    "model.fit(np.random.rand(32,16),np.ones(32),epochs=30, batch_size=32)\n",
    "# 训练完成后保存模型参数\n",
    "model.save('model.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练完成后，对新的数据进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73902476],\n",
       "       [0.7670607 ],\n",
       "       [0.7437851 ]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.random.rand(3,16))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 函数式 API \n",
    "函数式 API 用于构建更加复杂的网络结构，它允许构建任意的神经网络图，如多个输入或多个输出，这是 Sequential 顺序模型无法做到的。\n",
    "- 函数式 API 中网络层的实例是可调用的，它以张量为参数，并且返回一个张量\n",
    "- 输入和输出均为张量，它们都可以用来定义一个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.6377 - acc: 0.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a57f8e8f08>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "# 定义第一个输入层，Input 返回一个张量\n",
    "input1 = Input(shape=(8,))\n",
    "# 同理定义第二个输入层\n",
    "input2 = Input(shape=(16,))\n",
    "\n",
    "# 定义全连接层，接收输入层为参数，并返回一个张量\n",
    "x1 = Dense(64, activation='relu')(input1)\n",
    "x2 = Dense(64, activation='relu')(input2)\n",
    "\n",
    "# 连接两个输出\n",
    "x = concatenate([x1,x2])\n",
    "\n",
    "# 定义模型输出\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# 定义模型，这里创建了一个包含两个输入和一个输出的全连接模型\n",
    "model = Model(inputs=[input1, input2], outputs=output)\n",
    "\n",
    "# 与 Sequential 相同，配置学习过程\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy']) \n",
    "\n",
    "# 通过传递数组的方式进行训练\n",
    "model.fit([np.random.rand(32,8),np.random.rand(32,16)], np.ones(32))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 全连接层 Dense\n",
    "```python\n",
    "keras.layers.Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
    "```\n",
    "全连接层实现以下操作： `output = activation(dot(input, kernel) + bias)` 其中 `activation` 是按逐个元素计算的激活函数，`kernel` 是由网络层创建的权值矩阵，以及 `bias` 是其创建的偏置向量 (只在 `use_bias` 为 `True` 时才有用)。\n",
    "\n",
    "#### 参数设置\n",
    "- units: 正整数，节点数，即输出空间维度。\n",
    "- activation: 激活函数。 若不指定，则不使用激活函数。\n",
    "- use_bias: 布尔值，该层是否使用偏置向量。\n",
    "- kernel_initializer: kernel 权值矩阵的初始化器。\n",
    "- bias_initializer: 偏置向量的初始化器。\n",
    "- kernel_regularizer: 运用到 kernel 权值矩阵的正则化函数。\n",
    "- bias_regularizer: 运用到偏置向的的正则化函数。\n",
    "- activity_regularizer: 运用到层的输出的正则化函数。\n",
    "- kernel_constraint: 运用到 kernel 权值矩阵的约束函数。\n",
    "- bias_constraint: 运用到偏置向量的约束函数。\n",
    "\n",
    "#### 输出尺寸\n",
    "`(batch_size, ..., input_dim)` 的 nD 张量。\n",
    "\n",
    "#### 输出尺寸\n",
    "`(batch_size, ..., units)` 的 nD 张量。\n",
    "\n",
    "### 4. 1D 卷积层 Conv1D\n",
    "```python\n",
    "keras.layers.Conv1D(filters, kernel_size, strides=1, padding='valid', data_format='channels_last', dilation_rate=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
    "```\n",
    "1D 卷积层的卷积核以单维度上的层输入进行卷积，以生成输出张量。 如果 `use_bias` 为 `True`， 则会创建一个偏置向量并将其添加到输出中。 最后，如果 `activation` 不是 `None`，也会应用于输出。\n",
    "\n",
    "#### 参数设置\n",
    "filters: 整数，输出空间的维度，即卷积核的数量。\n",
    "kernel_size: 整数，或者单个整数表示的元组或列表，指明 1D 卷积窗口的长度。\n",
    "strides: 一个整数，或者单个整数表示的元组或列表， 指明卷积的步长。 \n",
    "padding: `\"valid\"`, `\"causal\"` 或 `\"same\"` 之一。`\"valid\"` 表示不填充。`\"same\"` 表示填充输入以使输出具有与原始输入相同的长度。  `\"causal\"` 表示因果（膨胀）卷积。\n",
    "data_format: 字符串,  `\"channels_last\"` (默认，TensorFlow 作为 backend 时使用) 或 `\"channels_first\"` 之一。\n",
    "dilation_rate: 一个整数，或者单个整数表示的元组或列表，指定用于膨胀卷积的膨胀率。\n",
    "activation: 要使用的激活函数，如未指定，则不使用激活函数。\n",
    "use_bias: 布尔值，该层是否使用偏置向量。\n",
    "kernel_initializer: kernel 权值矩阵的初始化器。\n",
    "bias_initializer: 偏置向量的初始化器。\n",
    "kernel_regularizer: 运用到 kernel 权值矩阵的正则化函数。\n",
    "bias_regularizer: 运用到偏置向量的正则化函数。\n",
    "activity_regularizer: 运用到层输出的正则化函数。\n",
    "kernel_constraint: 运用到 kernel 权值矩阵的约束函数。\n",
    "bias_constraint: 运用到偏置向量的约束函数。\n",
    "\n",
    "#### 输入尺寸\n",
    "`(batch_size, feature_len, input_dim)` 的 3D 张量\n",
    "\n",
    "#### 输出尺寸\n",
    "`(batch_size, new_feature_len, filters)` 的 3D 张量\n",
    "\n",
    "#### 例子 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 10, 12), (32, 8, 32))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Conv1D\n",
    "\n",
    "model = Sequential()\n",
    "# 模型将输入一个大小为 (batch, steps, input_dim) 的矩阵\n",
    "# 模型的输出尺度为 (batch, new_steps, filters)\n",
    "model.add(Conv1D(filters=32, kernel_size=3,input_shape=(10,12)))\n",
    "\n",
    "model.compile('rmsprop', 'mse')\n",
    "# 初始化一个 (32, 10, 12) 的矩阵 \n",
    "input_array = np.random.randn(32, 10, 12)\n",
    "output_array = model.predict(input_array)\n",
    "input_array.shape, output_array.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 嵌入层 Embedding\n",
    "```python\n",
    "keras.layers.Embedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None)\n",
    "```\n",
    "Embedding 层用于将正整数（索引值）转换为固定尺寸的稠密向量，常用于 NLP 模型的第一层，输入数据是用词袋模型等方法提取的正整数值。\n",
    "#### 参数设置\n",
    "- input_dim: int > 0。词汇表大小， 即最大整数 index + 1。\n",
    "- output_dim: int >= 0。词向量的维度。\n",
    "- embeddings_initializer: embeddings 矩阵的初始化方法。\n",
    "- embeddings_regularizer: embeddings matrix 的正则化方法。\n",
    "- embeddings_constraint: embeddings matrix 的约束函数。\n",
    "- mask_zero: 是否把 0 看作为一个应该被遮蔽的特殊的 \"padding\" 值。 \n",
    "- input_length: 输入序列的长度。如果需要连接 Flatten 和 Dense 层，则这个参数是必须的，不然 dense 层的输出尺寸就无法计算。\n",
    "\n",
    "#### 输入尺寸\n",
    "`(batch_size, sequence_length)` 的 2D 张量\n",
    "\n",
    "#### 输出尺寸\n",
    "`(batch_size, sequence_length, output_dim)` 的 3D 张量\n",
    "\n",
    "#### 例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 15), (4, 15, 8))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "model = Sequential()\n",
    "# 模型将输入一个大小为 (batch, input_length) 的整数矩阵，输入中最大的整数（即词索引）不应该大于 999 （词汇表大小）\n",
    "# 模型的输出尺度为 (batch, 15, 8)。\n",
    "model.add(Embedding(1000, 8, input_length=15))\n",
    "\n",
    "\n",
    "model.compile('rmsprop', 'mse')\n",
    "# 初始化一个 （4，15）的随机正整数矩阵作为输入\n",
    "input_array = np.random.randint(1000, size=(4, 15))\n",
    "output_array = model.predict(input_array)\n",
    "input_array.shape, output_array.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 长短期记忆网络层 LSTM\n",
    "```python\n",
    "keras.layers.LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False)\n",
    "```\n",
    "LSTM 是一种特殊的 RNN，能够学习长的依赖关系。 \n",
    "#### 参数设置\n",
    "- units: 正整数，输出空间的维度。\n",
    "- activation: 要使用的激活函数。 如果传入 None，则不使用激活函数。\n",
    "- recurrent_activation: 用于循环时间步的激活函数。 默认：分段线性近似 sigmoid。如果传入 None，则不使用激活函数。\n",
    "- use_bias: 布尔值，该层是否使用偏置向量。\n",
    "- kernel_initializer: kernel 权值矩阵的初始化器， 用于输入的线性转换。\n",
    "- recurrent_initializer: recurrent_kernel 权值矩阵 的初始化器，用于循环层状态的线性转换。\n",
    "- bias_initializer:偏置向量的初始化器。\n",
    "- unit_forget_bias: 布尔值。 如果为 True，初始化时，将忘记门的偏置加 1。 将其设置为 True 同时还会强制 bias_initializer=\"zeros\"。 \n",
    "- kernel_regularizer: 运用到 kernel 权值矩阵的正则化函数。\n",
    "- recurrent_regularizer: 运用到 recurrent_kernel 权值矩阵的正则化函数。\n",
    "- bias_regularizer: 运用到偏置向量的正则化函数。\n",
    "- activity_regularizer: 运用到层输出（它的激活值）的正则化函数。\n",
    "- kernel_constraint: 运用到 kernel 权值矩阵的约束函数。\n",
    "- recurrent_constraint: 运用到 recurrent_kernel 权值矩阵的约束函数。\n",
    "- bias_constraint: 运用到偏置向量的约束函数。\n",
    "- dropout: 在 0 和 1 之间的浮点数。 单元的丢弃比例，用于输入的线性转换。\n",
    "- recurrent_dropout: 在 0 和 1 之间的浮点数。 单元的丢弃比例，用于循环层状态的线性转换。\n",
    "- implementation: 实现模式，1 或 2。 模式 1 将把它的操作结构化为更多的小的点积和加法操作， 而模式 2 将把它们分批到更少，更大的操作中。 这些模式在不同的硬件和不同的应用中具有不同的性能配置文件。\n",
    "- return_sequences: 布尔值。是返回输出序列中的最后一个输出，还是全部序列。\n",
    "- return_state: 布尔值。除了输出之外是否返回最后一个状态。\n",
    "- go_backwards: 布尔值。默认 False，如果为 True，则向后处理输入序列并返回相反的序列。\n",
    "- stateful: 布尔值。默认 False， 如果为 True，则批次中索引 i 处的每个样品的最后状态 将用作下一批次中索引 i 样品的初始状态。\n",
    "- unroll: 布尔值。默认 False，如果为 True，则网络将展开，否则将使用符号循环。\n",
    "\n",
    "#### 输入尺寸\n",
    "`(batch_size, feature_len, input_dim)` 的 3D 张量\n",
    "#### 输出尺寸\n",
    "\n",
    "- 若 return_sequences 为 False，只输出序列最后一个值，为 `(batch_size, units)` 的 2D 张量\n",
    "- 若 return_sequences 为 True，输出全部序列，为 `(batch_size, new_feature_len,units)` 的 2D 张量\n",
    "#### 例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 10, 3), (4, 10, 1000))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import LSTM \n",
    "\n",
    "model = Sequential()\n",
    "# 模型将输入一个大小为 (batch, feature_len, input_dim) 的矩阵\n",
    "# 模型的输出尺度为 (batch, 1000)。\n",
    "model.add(LSTM(units=1000,return_sequences=True,input_shape=(10,3)))\n",
    "model.compile('rmsprop', 'mse')\n",
    "# 初始化一个 （4, 10, 3）的随机矩阵作为输入\n",
    "input_array = np.random.randn(4, 10, 3)\n",
    "output_array = model.predict(input_array)\n",
    "input_array.shape, output_array.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 双向封装器 Bidirectional\n",
    "```\n",
    "keras.layers.Bidirectional(layer, merge_mode='concat', weights=None)\n",
    "```\n",
    "RNN 的双向封装器，对序列进行前向和后向计算。\n",
    "\n",
    "#### 参数设置\n",
    "- layer: Recurrent 实例。\n",
    "- merge_mode: 前向和后向 RNN 的输出的结合模式。 为 {'sum', 'mul', 'concat', 'ave', None} 其中之一。 如果是 None，输出不会被结合，而是作为一个列表被返回。\n",
    "- weights: 要在 Bidirectional 模型中加载的初始权重\n",
    "\n",
    "#### 例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 5, 20)             1680      \n",
      "=================================================================\n",
      "Total params: 1,680\n",
      "Trainable params: 1,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional\n",
    "\n",
    "model = Sequential()\n",
    "# 封装 LTSM 层为 Bi-LSTM\n",
    "model.add(Bidirectional(LSTM(10, return_sequences=True),\n",
    "                        input_shape=(5, 10)))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "# 输出模型信息\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  三、总结\n",
    "在本演练中，学习了 Keras 的基本使用方法，在后续的演练中，将用 Keras 进行命名实体识别和关系抽取的任务。\n",
    "\n",
    "Keras 最初发行的时候，TensorFlow 还没有开源。那时的 Keras 主要使用的是 Theano 后端。2015年底 TensorFlow 开源后，Keras 才开始搭建TensorFlow 后端。如今 TensorFlow 是 Keras 最常用的后端。特别是在 TensorFlow 2.0 发行之后，TensorFlow 2.0 删除了篇冗余API，使用 Keras 和 eager execution 轻松构建模型。因此，文档中的 Keras 代码均可以用 `tf.keras` 进行无缝转换。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
