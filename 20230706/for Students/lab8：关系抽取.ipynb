{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 关系抽取"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、演练介绍\n",
    "### 1. 演练内容\n",
    "在上一演练中，将实体从文本中抽取出来，在实体抽取出来之后得到的是离散的节点，为了构成网状的知识，还需要从文本中提取实体之间的关系。因此，本演练中将演练如何从文本中抽取实体之间的关系。\n",
    "### 2. 演练技能点\n",
    "在本演练中，将接触\n",
    "- 关系抽取任务的定义与理论解决方法\n",
    "- 关系抽取任务中数据集处理方法\n",
    "- 关系抽取模型——PCNN 的理论知识\n",
    "- 用 Keras 构建 PCNN\n",
    "- PCNN 的训练与预测方法\n",
    "### 3. 演练要求\n",
    "本演练要求具备以下基本能力：\n",
    "- Keras 的基本使用方法\n",
    "\n",
    "## 二、演练原理\n",
    "\n",
    "### 1. 关系抽取任务\n",
    "\n",
    "在本演练中，关心的是有监督的关系抽取任务，即已知所有文本中包含的关系种类。此时关系抽取的任务形式就是一个文本分类的问题——任务的输入是一句话以及这句话中包含的两个实体，输出是关系类别。\n",
    "\n",
    "如文本“杨康，杨铁心与包惜弱之子，金国六王爷完颜洪烈的养子。”中，一共有四个人名实体，要获得“杨康”与“杨铁心”的关系，那么就要把“杨康”，“杨铁心”，“杨康，杨铁心与包惜弱之子，金国六王爷完颜洪烈的养子。” 这三个数据都输入到算法中。\n",
    "\n",
    "### 2. 数据预处理\n",
    "首先对数据进行位置编码，按句子中各个词离实体的距离进行编码。\n",
    "如“杨康，杨铁心与包惜弱之子，金国六王爷完颜洪烈的养子。”中，实体为“杨康”和“杨铁心”。然后记录句子中每个字与实体首字之间的距离。\n",
    "\n",
    "如\n",
    "\n",
    "|杨|康|，|杨|铁|心|与|包|惜|弱|之|子|，|金|国|六|王|爷|完|颜|洪|烈|的|养|子|。|\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "|0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|\n",
    "\n",
    "`pos_1=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25]`，0 就是杨康的起始位置\n",
    "\n",
    "|杨|康|，|杨|铁|心|与|包|惜|弱|之|子|，|金|国|六|王|爷|完|颜|洪|烈|的|养|子|。|\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "|-3|-2|-1|0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|\n",
    "\n",
    "`pos_2=[-3,-2,-1,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22]`，0 就是杨铁心的起始位置\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、演练步骤 \n",
    "先安装需要的包："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 如果已经安装过，可以不用再次安装，略过此步骤\n",
    "#!pip install tensorflow==1.13.1\n",
    "#!pip install keras==2.1.6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 数据处理\n",
    "原始文本和标签定义为"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['杨康', '杨铁心', '子女', '杨康，杨铁心与包惜弱之子，金国六王爷完颜洪烈的养子。'],\n",
       "  ['杨康', '杨铁心', '子女', '丘处机与杨铁心、郭啸天结识后，以勿忘“靖康之耻”替杨铁心的儿子杨康取名。'],\n",
       "  ['杨铁心', '包惜弱', '配偶', '金国六王爷完颜洪烈因为贪图杨铁心的妻子包惜弱的美色，杀害了郭靖的父亲郭啸天。'],\n",
       "  ['杨铁心', '包惜弱', '配偶', '杨康，杨铁心与包惜弱之子，金国六王爷完颜洪烈的养子。'],\n",
       "  ['张翠山', '殷素素', '配偶', '张无忌,武当七侠之一张翠山与天鹰教紫微堂主殷素素之子。'],\n",
       "  ['小龙女', '杨过', '师傅', '小龙女是杨过的师父，与杨过互生情愫，但因师生恋不容于世。'],\n",
       "  ['黄药师', '黄蓉', '父', '黄药师，黄蓉之父，对其妻冯氏（小字阿衡）一往情深。'],\n",
       "  ['郭啸天', '郭靖', '父', '郭靖之父郭啸天和其义弟杨铁心因被段天德陷害，死于临安牛家村。']],\n",
       " {'子女': 0, '配偶': 1, '师傅': 2, '父': 3})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对于 lists 中每一个子列表，第一个元素为实体1，第二个元素为实体2，第三个元素为实体1对实体2的关系，第四个元素为文本。\n",
    "lists = [['杨康','杨铁心','子女','杨康，杨铁心与包惜弱之子，金国六王爷完颜洪烈的养子。'],\n",
    "         ['杨康','杨铁心','子女','丘处机与杨铁心、郭啸天结识后，以勿忘“靖康之耻”替杨铁心的儿子杨康取名。'],\n",
    "         ['杨铁心','包惜弱','配偶','金国六王爷完颜洪烈因为贪图杨铁心的妻子包惜弱的美色，杀害了郭靖的父亲郭啸天。'],\n",
    "         ['杨铁心','包惜弱','配偶','杨康，杨铁心与包惜弱之子，金国六王爷完颜洪烈的养子。'],\n",
    "         ['张翠山','殷素素','配偶','张无忌,武当七侠之一张翠山与天鹰教紫微堂主殷素素之子。'],\n",
    "         ['小龙女','杨过','师傅','小龙女是杨过的师父，与杨过互生情愫，但因师生恋不容于世。'],\n",
    "         ['黄药师','黄蓉','父','黄药师，黄蓉之父，对其妻冯氏（小字阿衡）一往情深。'],\n",
    "         ['郭啸天','郭靖','父','郭靖之父郭啸天和其义弟杨铁心因被段天德陷害，死于临安牛家村。']]\n",
    "\n",
    "relation2idx = {'子女':0,'配偶':1,'师傅':2,'父':3}\n",
    "\n",
    "lists, relation2idx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，先将 `lists` 中的实体、关系与文本都单独拆分开来，并对文本进行位置编码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['杨',\n",
       "   '康',\n",
       "   '，',\n",
       "   '杨',\n",
       "   '铁',\n",
       "   '心',\n",
       "   '与',\n",
       "   '包',\n",
       "   '惜',\n",
       "   '弱',\n",
       "   '之',\n",
       "   '子',\n",
       "   '，',\n",
       "   '金',\n",
       "   '国',\n",
       "   '六',\n",
       "   '王',\n",
       "   '爷',\n",
       "   '完',\n",
       "   '颜',\n",
       "   '洪',\n",
       "   '烈',\n",
       "   '的',\n",
       "   '养',\n",
       "   '子',\n",
       "   '。'],\n",
       "  ['丘',\n",
       "   '处',\n",
       "   '机',\n",
       "   '与',\n",
       "   '杨',\n",
       "   '铁',\n",
       "   '心',\n",
       "   '、',\n",
       "   '郭',\n",
       "   '啸',\n",
       "   '天',\n",
       "   '结',\n",
       "   '识',\n",
       "   '后',\n",
       "   '，',\n",
       "   '以',\n",
       "   '勿',\n",
       "   '忘',\n",
       "   '“',\n",
       "   '靖',\n",
       "   '康',\n",
       "   '之',\n",
       "   '耻',\n",
       "   '”',\n",
       "   '替',\n",
       "   '杨',\n",
       "   '铁',\n",
       "   '心',\n",
       "   '的',\n",
       "   '儿',\n",
       "   '子',\n",
       "   '杨',\n",
       "   '康',\n",
       "   '取',\n",
       "   '名',\n",
       "   '。'],\n",
       "  ['金',\n",
       "   '国',\n",
       "   '六',\n",
       "   '王',\n",
       "   '爷',\n",
       "   '完',\n",
       "   '颜',\n",
       "   '洪',\n",
       "   '烈',\n",
       "   '因',\n",
       "   '为',\n",
       "   '贪',\n",
       "   '图',\n",
       "   '杨',\n",
       "   '铁',\n",
       "   '心',\n",
       "   '的',\n",
       "   '妻',\n",
       "   '子',\n",
       "   '包',\n",
       "   '惜',\n",
       "   '弱',\n",
       "   '的',\n",
       "   '美',\n",
       "   '色',\n",
       "   '，',\n",
       "   '杀',\n",
       "   '害',\n",
       "   '了',\n",
       "   '郭',\n",
       "   '靖',\n",
       "   '的',\n",
       "   '父',\n",
       "   '亲',\n",
       "   '郭',\n",
       "   '啸',\n",
       "   '天',\n",
       "   '。'],\n",
       "  ['杨',\n",
       "   '康',\n",
       "   '，',\n",
       "   '杨',\n",
       "   '铁',\n",
       "   '心',\n",
       "   '与',\n",
       "   '包',\n",
       "   '惜',\n",
       "   '弱',\n",
       "   '之',\n",
       "   '子',\n",
       "   '，',\n",
       "   '金',\n",
       "   '国',\n",
       "   '六',\n",
       "   '王',\n",
       "   '爷',\n",
       "   '完',\n",
       "   '颜',\n",
       "   '洪',\n",
       "   '烈',\n",
       "   '的',\n",
       "   '养',\n",
       "   '子',\n",
       "   '。'],\n",
       "  ['张',\n",
       "   '无',\n",
       "   '忌',\n",
       "   ',',\n",
       "   '武',\n",
       "   '当',\n",
       "   '七',\n",
       "   '侠',\n",
       "   '之',\n",
       "   '一',\n",
       "   '张',\n",
       "   '翠',\n",
       "   '山',\n",
       "   '与',\n",
       "   '天',\n",
       "   '鹰',\n",
       "   '教',\n",
       "   '紫',\n",
       "   '微',\n",
       "   '堂',\n",
       "   '主',\n",
       "   '殷',\n",
       "   '素',\n",
       "   '素',\n",
       "   '之',\n",
       "   '子',\n",
       "   '。'],\n",
       "  ['小',\n",
       "   '龙',\n",
       "   '女',\n",
       "   '是',\n",
       "   '杨',\n",
       "   '过',\n",
       "   '的',\n",
       "   '师',\n",
       "   '父',\n",
       "   '，',\n",
       "   '与',\n",
       "   '杨',\n",
       "   '过',\n",
       "   '互',\n",
       "   '生',\n",
       "   '情',\n",
       "   '愫',\n",
       "   '，',\n",
       "   '但',\n",
       "   '因',\n",
       "   '师',\n",
       "   '生',\n",
       "   '恋',\n",
       "   '不',\n",
       "   '容',\n",
       "   '于',\n",
       "   '世',\n",
       "   '。'],\n",
       "  ['黄',\n",
       "   '药',\n",
       "   '师',\n",
       "   '，',\n",
       "   '黄',\n",
       "   '蓉',\n",
       "   '之',\n",
       "   '父',\n",
       "   '，',\n",
       "   '对',\n",
       "   '其',\n",
       "   '妻',\n",
       "   '冯',\n",
       "   '氏',\n",
       "   '（',\n",
       "   '小',\n",
       "   '字',\n",
       "   '阿',\n",
       "   '衡',\n",
       "   '）',\n",
       "   '一',\n",
       "   '往',\n",
       "   '情',\n",
       "   '深',\n",
       "   '。'],\n",
       "  ['郭',\n",
       "   '靖',\n",
       "   '之',\n",
       "   '父',\n",
       "   '郭',\n",
       "   '啸',\n",
       "   '天',\n",
       "   '和',\n",
       "   '其',\n",
       "   '义',\n",
       "   '弟',\n",
       "   '杨',\n",
       "   '铁',\n",
       "   '心',\n",
       "   '因',\n",
       "   '被',\n",
       "   '段',\n",
       "   '天',\n",
       "   '德',\n",
       "   '陷',\n",
       "   '害',\n",
       "   '，',\n",
       "   '死',\n",
       "   '于',\n",
       "   '临',\n",
       "   '安',\n",
       "   '牛',\n",
       "   '家',\n",
       "   '村',\n",
       "   '。']],\n",
       " [0, 0, 1, 1, 1, 2, 3, 3],\n",
       " [[32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   48,\n",
       "   49,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   54,\n",
       "   55,\n",
       "   56,\n",
       "   57],\n",
       "  [1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36],\n",
       "  [19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   48,\n",
       "   49,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   54,\n",
       "   55,\n",
       "   56],\n",
       "  [29,\n",
       "   30,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   48,\n",
       "   49,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   54],\n",
       "  [22,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   48],\n",
       "  [32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   48,\n",
       "   49,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   54,\n",
       "   55,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59],\n",
       "  [32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   48,\n",
       "   49,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   54,\n",
       "   55,\n",
       "   56],\n",
       "  [28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   48,\n",
       "   49,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   54,\n",
       "   55,\n",
       "   56,\n",
       "   57]],\n",
       " [[29,\n",
       "   30,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   48,\n",
       "   49,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   54],\n",
       "  [28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   48,\n",
       "   49,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   54,\n",
       "   55,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63],\n",
       "  [13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   48,\n",
       "   49,\n",
       "   50],\n",
       "  [25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   48,\n",
       "   49,\n",
       "   50],\n",
       "  [11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37],\n",
       "  [28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   48,\n",
       "   49,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   54,\n",
       "   55],\n",
       "  [28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   48,\n",
       "   49,\n",
       "   50,\n",
       "   51,\n",
       "   52],\n",
       "  [32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   48,\n",
       "   49,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   54,\n",
       "   55,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas, labels, pos_list1, pos_list2 = [], [], [], []\n",
    "translation = 32\n",
    "for entity1, entity2, relation, text in lists:\n",
    "    # 找到第一个实体出现的下标\n",
    "    idx1 = text.index(entity1)\n",
    "    # 找到第二个实体出现的下标\n",
    "    idx2 = text.index(entity2)\n",
    "    sentence, pos1, pos2 = [], [], []\n",
    "    for i, w in enumerate(text):\n",
    "        sentence.append(w)\n",
    "        # 计算句子中每个字与实体1首字的距离\n",
    "        pos1.append(i-idx1+translation)\n",
    "        # 计算句子中每个字与实体2首字的距离\n",
    "        pos2.append(i-idx2+translation)\n",
    "    datas.append(sentence)\n",
    "    labels.append(relation2idx[relation])\n",
    "    pos_list1.append(pos1)\n",
    "    pos_list2.append(pos2)\n",
    "\n",
    "datas, labels, pos_list1, pos_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({'杨': 11,\n",
       "          '康': 4,\n",
       "          '，': 11,\n",
       "          '铁': 6,\n",
       "          '心': 6,\n",
       "          '与': 5,\n",
       "          '包': 3,\n",
       "          '惜': 3,\n",
       "          '弱': 3,\n",
       "          '之': 7,\n",
       "          '子': 7,\n",
       "          '金': 3,\n",
       "          '国': 3,\n",
       "          '六': 3,\n",
       "          '王': 3,\n",
       "          '爷': 3,\n",
       "          '完': 3,\n",
       "          '颜': 3,\n",
       "          '洪': 3,\n",
       "          '烈': 3,\n",
       "          '的': 7,\n",
       "          '养': 2,\n",
       "          '。': 8,\n",
       "          '丘': 1,\n",
       "          '处': 1,\n",
       "          '机': 1,\n",
       "          '、': 1,\n",
       "          '郭': 5,\n",
       "          '啸': 3,\n",
       "          '天': 5,\n",
       "          '结': 1,\n",
       "          '识': 1,\n",
       "          '后': 1,\n",
       "          '以': 1,\n",
       "          '勿': 1,\n",
       "          '忘': 1,\n",
       "          '“': 1,\n",
       "          '靖': 3,\n",
       "          '耻': 1,\n",
       "          '”': 1,\n",
       "          '替': 1,\n",
       "          '儿': 1,\n",
       "          '取': 1,\n",
       "          '名': 1,\n",
       "          '因': 3,\n",
       "          '为': 1,\n",
       "          '贪': 1,\n",
       "          '图': 1,\n",
       "          '妻': 2,\n",
       "          '美': 1,\n",
       "          '色': 1,\n",
       "          '杀': 1,\n",
       "          '害': 2,\n",
       "          '了': 1,\n",
       "          '父': 4,\n",
       "          '亲': 1,\n",
       "          '张': 2,\n",
       "          '无': 1,\n",
       "          '忌': 1,\n",
       "          ',': 1,\n",
       "          '武': 1,\n",
       "          '当': 1,\n",
       "          '七': 1,\n",
       "          '侠': 1,\n",
       "          '一': 2,\n",
       "          '翠': 1,\n",
       "          '山': 1,\n",
       "          '鹰': 1,\n",
       "          '教': 1,\n",
       "          '紫': 1,\n",
       "          '微': 1,\n",
       "          '堂': 1,\n",
       "          '主': 1,\n",
       "          '殷': 1,\n",
       "          '素': 2,\n",
       "          '小': 2,\n",
       "          '龙': 1,\n",
       "          '女': 1,\n",
       "          '是': 1,\n",
       "          '过': 2,\n",
       "          '师': 3,\n",
       "          '互': 1,\n",
       "          '生': 2,\n",
       "          '情': 2,\n",
       "          '愫': 1,\n",
       "          '但': 1,\n",
       "          '恋': 1,\n",
       "          '不': 1,\n",
       "          '容': 1,\n",
       "          '于': 2,\n",
       "          '世': 1,\n",
       "          '黄': 2,\n",
       "          '药': 1,\n",
       "          '蓉': 1,\n",
       "          '对': 1,\n",
       "          '其': 2,\n",
       "          '冯': 1,\n",
       "          '氏': 1,\n",
       "          '（': 1,\n",
       "          '字': 1,\n",
       "          '阿': 1,\n",
       "          '衡': 1,\n",
       "          '）': 1,\n",
       "          '往': 1,\n",
       "          '深': 1,\n",
       "          '和': 1,\n",
       "          '义': 1,\n",
       "          '弟': 1,\n",
       "          '被': 1,\n",
       "          '段': 1,\n",
       "          '德': 1,\n",
       "          '陷': 1,\n",
       "          '死': 1,\n",
       "          '临': 1,\n",
       "          '安': 1,\n",
       "          '牛': 1,\n",
       "          '家': 1,\n",
       "          '村': 1}),\n",
       " ['杨',\n",
       "  '康',\n",
       "  '，',\n",
       "  '铁',\n",
       "  '心',\n",
       "  '与',\n",
       "  '包',\n",
       "  '惜',\n",
       "  '弱',\n",
       "  '之',\n",
       "  '子',\n",
       "  '金',\n",
       "  '国',\n",
       "  '六',\n",
       "  '王',\n",
       "  '爷',\n",
       "  '完',\n",
       "  '颜',\n",
       "  '洪',\n",
       "  '烈',\n",
       "  '的',\n",
       "  '养',\n",
       "  '。',\n",
       "  '郭',\n",
       "  '啸',\n",
       "  '天',\n",
       "  '靖',\n",
       "  '因',\n",
       "  '妻',\n",
       "  '害',\n",
       "  '父',\n",
       "  '张',\n",
       "  '一',\n",
       "  '素',\n",
       "  '小',\n",
       "  '过',\n",
       "  '师',\n",
       "  '生',\n",
       "  '情',\n",
       "  '于',\n",
       "  '黄',\n",
       "  '其'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# 统计每个字出现的次数, sum(datas,[]) 的功能是将列表铺平\n",
    "word_counts = Counter(sum(datas, []))\n",
    "# 建立字典表，只记录出现次数不小于 2 的字\n",
    "vocab = [w for w, f in iter(word_counts.items()) if f >= 2]\n",
    "word_counts, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'杨': 2,\n",
       " '康': 3,\n",
       " '，': 4,\n",
       " '铁': 5,\n",
       " '心': 6,\n",
       " '与': 7,\n",
       " '包': 8,\n",
       " '惜': 9,\n",
       " '弱': 10,\n",
       " '之': 11,\n",
       " '子': 12,\n",
       " '金': 13,\n",
       " '国': 14,\n",
       " '六': 15,\n",
       " '王': 16,\n",
       " '爷': 17,\n",
       " '完': 18,\n",
       " '颜': 19,\n",
       " '洪': 20,\n",
       " '烈': 21,\n",
       " '的': 22,\n",
       " '养': 23,\n",
       " '。': 24,\n",
       " '郭': 25,\n",
       " '啸': 26,\n",
       " '天': 27,\n",
       " '靖': 28,\n",
       " '因': 29,\n",
       " '妻': 30,\n",
       " '害': 31,\n",
       " '父': 32,\n",
       " '张': 33,\n",
       " '一': 34,\n",
       " '素': 35,\n",
       " '小': 36,\n",
       " '过': 37,\n",
       " '师': 38,\n",
       " '生': 39,\n",
       " '情': 40,\n",
       " '于': 41,\n",
       " '黄': 42,\n",
       " '其': 43}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建词袋模型，和上一演练相同，将字典从 2 开始编号，把 0 和 1 空出来，0 作为填充元素，1 作为不在字典中的字的编号\n",
    "word2idx = dict((w,i+2) for i,w in enumerate(vocab))\n",
    "word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "f:\\Venvs\\kg01\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "f:\\Venvs\\kg01\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "f:\\Venvs\\kg01\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "f:\\Venvs\\kg01\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "f:\\Venvs\\kg01\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "f:\\Venvs\\kg01\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((8, 64), (8, 4), (8, 64), (8, 64))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# 构建输入，即对于样本中每一个字，从词袋模型中找到这个字对应的 idx，出现频率过低的字，并没有出现在词袋模型中，此时将这些字的 idx 取为 1\n",
    "train_x = [[word2idx.get(w, 1) for w in s] for s in datas]\n",
    "\n",
    "max_len = 64\n",
    "\n",
    "# 在输入的左边填充 0\n",
    "train_x = pad_sequences(train_x, max_len, value=0)\n",
    "## 填充位置编码\n",
    "train_pos1 = pad_sequences(pos_list1, max_len, value=0)\n",
    "train_pos2 = pad_sequences(pos_list2, max_len, value=0)\n",
    "# one_hot 编码 label\n",
    "train_y = to_categorical(labels, num_classes=len(relation2idx))\n",
    "\n",
    "train_x.shape, train_y.shape, train_pos1.shape, train_pos2.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 构建网络模型\n",
    "因为网络有多个输入：文本与位置编码，属于复杂模型，因此这里使用 Keras 的函数式 API 来定义网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From f:\\Venvs\\kg01\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 64, 16)       4096        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 64, 16)       4096        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 64, 16)       4096        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64, 32)       0           embedding_3[0][0]                \n",
      "                                                                 embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64, 32)       0           embedding_3[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 62, 128)      12416       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 62, 128)      12416       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 256)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 4)            1028        concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 38,148\n",
      "Trainable params: 38,148\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Embedding, concatenate, Conv1D, GlobalMaxPool1D, Dense, LSTM\n",
    "from keras.models import Model\n",
    "\n",
    "# 定义输入层\n",
    "words = Input(shape=(max_len,),dtype='int32')\n",
    "position1 = Input(shape=(max_len,),dtype='int32')\n",
    "position2 = Input(shape=(max_len,),dtype='int32')\n",
    "#  Embedding 层将输入进行编码\n",
    "pos_emb1 = Embedding(output_dim=16, input_dim=256)(position1)\n",
    "pos_emb2 = Embedding(output_dim=16, input_dim=256)(position2)\n",
    "word_emb = Embedding(output_dim=16, input_dim=256)(words)\n",
    "# 分别拼接 文本编码与位置1 和文本编码与位置2\n",
    "concat1 = concatenate([word_emb, pos_emb1])\n",
    "concat2 = concatenate([word_emb, pos_emb2])\n",
    "# 卷积池化层\n",
    "conv1 = Conv1D(filters=128, kernel_size=3)(concat1)\n",
    "pool1 = GlobalMaxPool1D()(conv1)\n",
    "conv2 = Conv1D(filters=128, kernel_size=3)(concat2)\n",
    "pool2 = GlobalMaxPool1D()(conv2)\n",
    "# 拼接，最后接全连接层，激活函数为 softmax\n",
    "concat = concatenate([pool1, pool2])\n",
    "out = Dense(units=len(relation2idx),activation='softmax')(concat)\n",
    "\n",
    "model = Model(inputs=[words, position1, position2],outputs=out)\n",
    "# 编译模型\n",
    "model.compile(optimizer='ADAM', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From f:\\Venvs\\kg01\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From f:\\Venvs\\kg01\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.3853 - acc: 0.2500\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 487us/step - loss: 1.3583 - acc: 0.7500\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 375us/step - loss: 1.3339 - acc: 0.8750\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 312us/step - loss: 1.3115 - acc: 0.7500\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 526us/step - loss: 1.2906 - acc: 0.5000\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 499us/step - loss: 1.2708 - acc: 0.5000\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 376us/step - loss: 1.2521 - acc: 0.5000\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 499us/step - loss: 1.2340 - acc: 0.5000\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 376us/step - loss: 1.2163 - acc: 0.3750\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 500us/step - loss: 1.1992 - acc: 0.3750\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 632us/step - loss: 1.1823 - acc: 0.3750\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 500us/step - loss: 1.1658 - acc: 0.3750\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 423us/step - loss: 1.1491 - acc: 0.3750\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 375us/step - loss: 1.1322 - acc: 0.3750\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 520us/step - loss: 1.1154 - acc: 0.3750\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 365us/step - loss: 1.0985 - acc: 0.5000\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 372us/step - loss: 1.0815 - acc: 0.5000\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 431us/step - loss: 1.0643 - acc: 0.5000\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 462us/step - loss: 1.0469 - acc: 0.5000\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 252us/step - loss: 1.0292 - acc: 0.7500\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 501us/step - loss: 1.0112 - acc: 0.7500\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 377us/step - loss: 0.9928 - acc: 0.7500\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 250us/step - loss: 0.9738 - acc: 0.7500\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 375us/step - loss: 0.9546 - acc: 0.8750\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 431us/step - loss: 0.9349 - acc: 0.8750\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 249us/step - loss: 0.9146 - acc: 0.8750\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.8938 - acc: 0.8750\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 316us/step - loss: 0.8725 - acc: 0.8750\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.8507 - acc: 0.8750\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 386us/step - loss: 0.8284 - acc: 0.8750\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 406us/step - loss: 0.8057 - acc: 0.8750\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 375us/step - loss: 0.7826 - acc: 0.8750\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 320us/step - loss: 0.7591 - acc: 0.8750\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 376us/step - loss: 0.7351 - acc: 0.8750\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 373us/step - loss: 0.7107 - acc: 0.8750\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 380us/step - loss: 0.6861 - acc: 0.8750\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 376us/step - loss: 0.6613 - acc: 0.8750\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 248us/step - loss: 0.6362 - acc: 1.0000\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 315us/step - loss: 0.6110 - acc: 1.0000\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 263us/step - loss: 0.5856 - acc: 1.0000\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.5602 - acc: 1.0000\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 254us/step - loss: 0.5349 - acc: 1.0000\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.5096 - acc: 1.0000\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 440us/step - loss: 0.4846 - acc: 1.0000\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 251us/step - loss: 0.4599 - acc: 1.0000\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 375us/step - loss: 0.4355 - acc: 1.0000\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 376us/step - loss: 0.4116 - acc: 1.0000\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 248us/step - loss: 0.3882 - acc: 1.0000\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 500us/step - loss: 0.3655 - acc: 1.0000\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.3434 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# 训练 50 次\n",
    "model.fit([train_x, train_pos1, train_pos2], train_y, batch_size=8, epochs=50)\n",
    "model.save('model.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 模型预测 \n",
    "在这里使用训练集中的一个实例进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('张翠山', '殷素素', '张无忌,武当七侠之一张翠山与天鹰教紫微堂主殷素素之子。')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_instance = ['张翠山','殷素素','张无忌,武当七侠之一张翠山与天鹰教紫微堂主殷素素之子。']\n",
    "test_ne1, test_ne2, test_text = test_instance\n",
    "test_ne1, test_ne2, test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0, 33,  1,  1,  1,  1,  1,  1,  1, 11, 34, 33,\n",
       "          1,  1,  7, 27,  1,  1,  1,  1,  1,  1,  1, 35, 35, 11, 12, 24]]),\n",
       " array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,\n",
       "         33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48]]),\n",
       " array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21,\n",
       "         22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将预测数据转换为向量\n",
    "pred_x = [word2idx.get(w, 1) for w in test_text]\n",
    "idx1 = test_text.index(test_ne1)\n",
    "idx2 = test_text.index(test_ne2)\n",
    "pos1 = [i-idx1+translation for i in range(len(test_text))]\n",
    "pos2 = [i-idx2+translation for i in range(len(test_text))]\n",
    "pred_x = pad_sequences([pred_x], max_len, value=0)\n",
    "test_pos1 = pad_sequences([pos1], max_len, value=0)\n",
    "test_pos2 = pad_sequences([pos2], max_len, value=0)\n",
    "pred_x, test_pos1, test_pos2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.07006253, 0.7985946 , 0.04585258, 0.08549027]], dtype=float32),\n",
       " 1,\n",
       " '配偶')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 翻转 relation2idx 字典\n",
    "idx2relation = dict(zip(relation2idx.values(),relation2idx.keys()))\n",
    "# 使用模型进行预测\n",
    "pred = model.predict([pred_x, test_pos1, test_pos2])\n",
    "# 模型预测最大值的位置作为预测值\n",
    "output_idx = np.argmax(pred)\n",
    "# 找到 idx2relation 中实际的标签\n",
    "output_label = idx2relation[output_idx]\n",
    "pred, output_idx, output_label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 总结 \n",
    "在本演练中，演示了关系抽取任务的定义，并使用了一个小型的神经网络来实现关系抽取任务中的数据处理、训练与预测。\n",
    "\n",
    "由于小型的神经网络的参数量较少，拟合能力有限，从而随着训练数据量的增加就会出现欠拟合的现象。因此，用少量数据集在小型网络上训练完成后，再逐渐增大数据量，同时将小型网络复杂化，如使用现有的 [PCNN](http://www.emnlp2015.org/proceedings/EMNLP/pdf/EMNLP203.pdf)，[Attention-BiLSTM](https://www.aclweb.org/anthology/P16-2034) 等用于关系抽取的经典神经网络结构，将任务的精度提升到想要的结果。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
