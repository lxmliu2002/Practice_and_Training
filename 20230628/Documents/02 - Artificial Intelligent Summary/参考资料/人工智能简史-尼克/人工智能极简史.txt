人工智能简史
前期，人工智能使用知识（句法、逻辑等规则）来搜索问题的解，主要用在数学和逻辑领域，比如证明数学定理、计算函数、逻辑推理等。中期，人工智能强方法盛行，在具体的领域有独立地应用。当下，人工智能正以神经网络、概率和决策等连接主义理论为基础，以机器学习、数据挖掘等为工具，迈向产业性强、覆盖面广的发展方向，将会深刻影响人们的生活方式。

1 孕育期（1943-1955）
    1) Warren McCulloch和Walter Pitts在1943年提出了一种人工神经元模型，并且证明了任何逻辑连接词（与、或、非等）和可计算的函数都可以通过相连神经元的某个网络来实现。
    2) 唐纳德·赫布（Donald Hebb）在1949年展示了一条简单的用于修改神经元之间的连接强度的更新规则，现在称为赫布型学习。
    3) 两名哈佛大学的本科生，马文·明思基（Marvin Minsky）和Dean Edmonds，在1950年建造了第一台神经网络计算机，称为SNARC。明思基还研究了神经网络中的一般计算。
    4) 阿兰·图灵在1947年发表了人工智能主题演讲，并在1950年的文章“计算机器与智能（Computing Machinery and Intelligence）”中清晰地表达了有说服力的应办之事：图灵测试、机器学习、遗传算法和强化学习，还提出了儿童程序（Child Programme）的思想。
    
2 诞生（1956）
    1) 1956年夏天，约翰·麦卡锡组织了十位对自动机理论、神经网络和智能研究感兴趣的研究者们召集在一起，在达特茅斯组织了一个为期两个月的研讨会，被认为是人工智能领域的诞生标志，达特茅斯大学也成为了公认的人工智能领域的诞生地。
    
3 第一轮循环成功：早期的热情，巨大的期望（1956-1969）
    1) 早期人工智能在有限的方面充满成功；
    2) 通用问题求解器或GPS继承并发扬了纽厄尔和西蒙的早期成就（逻辑推理程序），他们构想出著名的“物理符号系统”假设；
    3) 定理证明
        1956年,CMU的LT程序证明了数学家罗素所著<数学原理>第二章的38条定理
        1959年,洛克菲勒大学教授王浩使用"王算法",证明了<数学原理>全部350条定理
        1963年,CMU改进的LT程序证明了<数学原理>第二章的全部52条定理,改程序其后被改进成GPS
    4) 在IBM，内森尼尔·罗切斯特和他的同事们制作了一些最初的人工智能程序，如几何定理证明器、西洋跳棋程序（驳斥了计算机只能做被告知的事的思想）；
    5) 约翰·麦卡锡从达特茅斯搬到了MIT，在1958年做出了三项至关重要的贡献：①定义了高级语言Lisp，在后来的30年中成为了占统治地位的人工智能编程语言；②发明了分时技术；③发表了题为“有常识的程序”（Programs with Common Sence）的论文，文中描述了意见接受者（Advice Taker），这个假想程序可以被看成是第一个完整的人工智能系统；
    6) 1963年，麦卡锡在斯坦福创办了人工智能实验室。1965年，J.A.Robinson归结方法的发现促进了麦卡锡使用逻辑来建造最终的意见接受者的计划；
    7) 明斯基指导了一系列学生，研究了诸如闭合式微积分问题、几何类推问题、代数问题等微观世界问题。最著名的微观世界是积木世界；
    8) 基于McCulloch和Pitts的神经网络的早期工作也十分兴旺。
    最核心的是逻辑主义（符号主义），功能主义占主流。主要是用机器证明的办法去证明和推理一些知识，比如能不能用机器证明一个数学定理，这是机器证明的问题。要想证明这些问题，需要把原来的条件和定义从形式化变成逻辑表达，然后用逻辑的方法去证明最后的结论是对的还是错的，叫做逻辑证明。
    领头羊：斯坦福大学、CMU、MIT、IBM、哈佛大学。

4 第一轮循环失望：现实的困难（1966-1973）
    1) 早期人工智能系统在简单实例上令人鼓舞的性能使研究者们过于自信，然而，当用于更宽的问题选择和更难的问题时，结果证明都非常失败；
    2) 第一种困难：早期程序对其主题一无所知，仅依靠简单的句法、逻辑等规则的使用获得成功；
    3) 第二种困难：人工智能试图求解的问题的难解性，放大到更大更复杂的问题不只是更快的硬件和更大的存储器。
    4) 第三种困难：用来产生智能行为的基本结构的某些根本局限。比如，明斯基证明了：感知机能够学会他们能表示的任何东西，但是它们能学会的东西很少。

5 基于知识的系统（1969-1979）
    1) 上一轮的成功和失望，皆因为AI研究采用的是通用的搜索机制，试图串联基本的推理步骤来寻找完全解，称为弱方法。不能扩展到大规模的或困难的问题实例；
    2) 强方法：使用更强有力的、领域相关的知识，以允许更大量的推理步骤，且可以更容易地处理狭窄的专门领域里发生的典型情况。如：DENDRAL程序，根据质谱仪提供的信息推断分子结构的问题；医疗诊断，诊断血液传染；特定领域的自然语言理解。
    3) 逻辑程序语言: Prolog
    4) 产生式系统
        一个综合DB + 一组产生式规则 + 一个控制系统
    标志性基石：在统计方法中引入符号方法，进行语义处理，出现了基于知识的方法，人机交互成为可能。
    研究重点:规则表示, 规则生成, 谓词演算, 知识获取, 搜索算法, 启发式搜索
    实际上早期的计算机人工智能都是沿着这条路在走，当时有很多专家系统，比如医学专家系统。医学专家系统输入的是什么呢？是输入一些症状。这些症状是用语言输进的，但机器里面可以变换成逻辑表达，用符号演算的办法推出来你大概是什么病或者肯定是什么病。所以当时在逻辑的抽象、逻辑的运算和逻辑表达方面，人们花了大量的工夫。
    当时在1958年这个领域刚开了两年之后，就有两位计算机领域的大师，Herbert Simon和Allen Newell，他们的理论直接涉及到决策论，完全是一套逻辑主义的推理方法。他们对决策论经济学界很看好，很多人认为做经济学的判断是非常应该的。当时他们做了一个大胆的预言，十年之内计算机就可以写出优美的乐谱，十年之内计算机就能够实现大多数的心理学行为。当时他们判断这些事都可以迎刃而解，并不是什么问题，但事实证明这不是真的。包括国际象棋冠军一直到一九九几年才实现，围棋一直到去年才实现。

    数学定理这件事是做通了，因为这件事是所有的事当中最容易用逻辑的办法解决的，所以学习推理证明是判断的十个可能有四个可能最贴近目标。计算机自动谱曲这件事，当然可以做很多，但是并不能达到随心所欲的程度。最后这个心理学行为，到现在也还没有完全做好。
    定理证明实际上是第一个浪潮当中实现效果最好的，当时有很多数学家用定理思路证明了数学定理。配合这些工作，当时出了很多和逻辑证明相关的计算机，叫做逻辑程序语言，比如很知名的Prolog。最关键的是要有一个很好的数据库，要有一个控制系统，进行逻辑推理和演算。

    1976年前后，由于四大预言实现遥遥无期，关于人工智能方法论的争论风声渐紧。1977年，曾是Simon研究生的Feigenbaum提出知识工程的概念。在一开始逻辑主义和连接主义都在，第一个浪潮当中逻辑主义是完全占上风的，连接主义那时候不太吃香。然而逻辑主义最后没有实现目标，引起了大家的反思，这时候神经系统就慢慢占了上风。
    
6 成为产业（1980—）
    1) 第一个成功的商用专家系统R1开始在数据设备公司（DEC）运转，该程序帮助为新计算机系统配置订单；
    2) 总的来说，AI产业从1980年的区区几百万美元暴涨到1988年的数十亿美元，包括几百家公司研发的专家系统、视觉系统、机器人以及服务这些目标的专门软件和硬件。

7 神经网络的回归（1986—）
    0) 1975年,Paul Werbos提出了 Backpropagation algorithm(BP算法), 使得多层人工神经元网络的学习变为可能.
    1) 在20世纪80年代中期，至少4个不同的研究组重新发明了由Bryson和Ho于1969年首次建立的反传（BP神经网络）学习算法，属于联结主义模型；
        联结主义盛行, 深度学习尚未突破. 
    2) 当前观点认为，连接主义方法和符号主义方法（纽厄尔、西蒙和麦卡锡等人主张的方法）是互补的，不是竞争的；
    3) 现代神经网络研究分离成了两个领域：一个是建立有效的网络结构和算法并理解它们的数学属性，另一个关心的是对实际神经元的实验特性和神经元的集成建模。
    4) 
    在70年代末，整个神经元网络、模型都有突飞猛进的成绩，最重要的是有一个叫BP网络，这个模型能够解决神经元网络的学习。以前一个刺激对应一个输出，刺激和输出是一对，有什么样的刺激就有什么样的输出。1986年BP网络证明了神经元网络，后来大家往更大的领域应用，做出了比较大的贡献。后来在很多模式识别的领域、手写汉字的识别、字符识别、简单的人脸识别才开始慢慢用起来，这个领域一下就热起来。
    连接主义持续了十几年，从1976年到80年代中期属于低潮，大概到2006年又开始走下坡了。1986年BP网络刚出来的时候解决了不少问题，大家都认为人工智能是有希望的，后来十几年以后发现神经元网络解决单一问题可以，解决复杂问题不行。训练学习的时候，数据量太大，有很多结果到一定程度就不再往上升了。
    
8 采用科学的方法（1987—）
    1) 在方法论方面，AI最终成为坚实的科学方法：假设必须遵从严格的经验实验，结果的重要性必须经过统计分析；
    2) 语音识别和机器翻译领域都发生了方法论的变革：隐马尔可夫模型（HMMs）主导前者，基于单词序列的方法主导后者；
    3) 通过改进的方法论和理论框架，神经网络领域达到一个新的理解程度，可以和统计学、模式识别和机器学习等领域的对应技术相提并论；
    4) 《概率》和《智能系统中的概率推理》开启了AI对概率和决策理论的新一轮接纳：贝叶斯网络对于不确定知识进行有效表示和严格推理，主导着不确定推理和专家系统中的AI研究，结合了经典AI和神经网络的最好部分
    5) 类似的温和革命也发生在机器人、计算机视觉和知识表示领域。

9 智能agent的出现（1995—）
    1) 研究者们再一次审视“完整Agent”问题。Allen NeWell、John Laird和Paul Rosenbloom在SOAR系统上的工作是最有名的完整Agent结构的例子，智能Agent最重要的环境之一就是Internet；
    2) 试图建立完整Agent，需要把以前被孤立的AI子领域重新组织；
    3) 一些有影响的AI创建者，包括John McCarthy、Marvin Minsky、Nils Nilsson和Patrick Winston都表达了对AI进展的不满。他们认为AI应该少把重点放在改进对特定任务表现很好的应用，例如驾驶汽车、下棋或者语言识别。转而，应该回到它的根：会思考、会学习、会创造的人类级AI（HLAI）；
    4) 和HLAI相关的思想是人工通用智能（AGI）子领域，寻找通用的在任何环境中的学习和行动算法。
    
10 极大数据集的可用性（2001—）
    1) 采用更多训练数据带来的性能提升超过选用算法带来的性能提升；
    2) Yarowsky在1995年的论文提到，在词语歧义消除方面的工作，一个普通算法使用一亿个单词的未标注训练数据，会好过最有名的算法使用100万个单词；
    3) Hays和Efros在2007年讨论了照片中补洞的问题，发现如果他们只用一万张照片，那么他们的算法性能会很差，但如果增加到两百万张时，算法会一跃而表现出极好的性能。
    把一些技术、神经元网络和统计的方法结合在一起。最初人们并不知道到底要怎么做，2006年辛顿的一篇文章，认为现在的神经元系统能做到几千层都没有问题，有点类似BP网络。
    深度学习的成功，既有硬件的进步，也有卷积神经网络模型与参数训练技巧的进步。
    它的弱点是整个网络可以做得很深，也很容易去训练，但是训练得出的结果和人是完全不一样的。因为人脑里面是有非常明确的定义，很容易举一反三推理，但是神经元系统不行。神经元网络本身实际它的物理意义没有了，怎么样把神经元网络和真人的智能概念的理解举一反三的能力运用起来，是它天然的障碍。这个障碍的解法在哪里，理论上是统计学的方法，现在神经元网络基本上更多的是靠连接来实现的这个功能，但不是靠统计来做。人们希望把连接和深层次的统计结合在一起，才有可能走出现在的环境。

11 最新发展
    无人驾驶汽车、语音识别、自主规划与调度、博弈、垃圾信息过滤、后勤规划、机器人技术、机器翻译等。